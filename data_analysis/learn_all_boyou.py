import sys, os, json, random, logging, time, socket
import numpy as np
from itertools import cycle
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import precision_recall_fscore_support
from matplotlib import gridspec
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.gaussian_process.kernels import RationalQuadratic
from sklearn.gaussian_process.kernels import Matern
from sklearn.gaussian_process.kernels import ExpSineSquared
from sklearn.gaussian_process.kernels import DotProduct
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.linear_model import LogisticRegression
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from matplotlib.backends.backend_pdf import PdfPages
from scipy import interp
from sklearn.preprocessing import MinMaxScaler
from sklearn import manifold
import matplotlib.pylab as pylab
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict

class boyou_learn:

    def __init__ (self, logger, data_path_prefix='./'):
        """
        init function for initiating the members
        """

        self.benign_data    = []
        self.malware_data   = []
        self.logger         = logger
        self.benign_dict    = {}
        self.malware_dict   = {}
        if socket.gethostname() == 'scc2':
            self.data_path_prefix = '../amd_data/'
        else:
            self.data_path_prefix = data_path_prefix
        self.data_path_prefix = data_path_prefix
        self.result_path    = './'

        # setup seed
        random.seed(os.urandom(100))

    def load_data(self):
        """
        The load_data function loads data according to a list of files specified in the 
        list. Benignware data are vertically stacked, and it is non-discriminative towards
        different binaries. It initiates the report of current experiment.

        :param dict self.report: The report is the dictionary of all the results.
        :param dict self.benign_dict: The benignware names are recorded. \
        The value of each entry is boolean. The purpose of this is to keep track of benignware.
        :param dict self.malware_dict: Similar to benignware, this works for malware.
        :param array self.benign_data: Every feature vector in the numpy array is vertically stacked.
        :param array self.malware_data: Similar to self.benign_data
        :param list self.benign_label: Labels for benignware [1, 1, 1, ... 1]
        :param list self.malware_label: Similar to benign_label [0, 0, 0, ... 0]
        :param array self.data: Stacking benign data over malware data
        :param array self.label: Stacking benign label over malware label
    
        """


        self.report = {}

        benign_files = ['amd_benign_metadata/benign_post_pca_data.txt',\
                'amd_desktop_metadata/benign_post_pca_data.txt',\
                'amd_python_benign_metadata/python_benign_post_pca_data.txt'
                ]
        malware_files = ['amd_malware_metadata/malware_post_pca_data.txt']
        
        for files in benign_files:
            self.logger.info("Loading " + self.data_path_prefix + files + " ... ")
            with open(self.data_path_prefix + files, 'r') as data_file:
                json_data = json.load(data_file)

            for bm in json_data.keys():
                if bm not in self.benign_dict:
                    self.logger.info("benign bm " + bm)
                self.benign_dict[bm]        = False
                for each_sample in json_data[bm]:
                    if self.benign_data == []:
                        self.benign_data = np.array(each_sample) 
                    else:
                        self.benign_data = np.vstack((self.benign_data, each_sample))

            self.logger.info("benign bm numbers " + str(len(self.benign_dict.keys())))


            data_file.close() 

        for files in malware_files:
            self.logger.info("Loading " + self.data_path_prefix + files + " ... ")
            with open(self.data_path_prefix + files, 'r') as data_file:
                json_data = json.load(data_file)

            for bm in json_data.keys():
                self.malware_dict[bm]       = False
                for each_sample in json_data[bm]:
                    if self.malware_data == []:
                        self.malware_data = np.array(each_sample) 
                    else:
                        try:
                            self.malware_data = np.vstack((self.malware_data, each_sample))
                        except:
                            continue

            data_file.close() 

        self.benign_label   = np.ones(len(self.benign_data))
        self.malware_label  = np.zeros(len(self.malware_data))
        self.logger.info("Benign Size " + str(len(self.benign_label)))
        self.logger.info("Malware Size " + str(len(self.malware_label)))

        self.sample_size    = min(len(self.benign_label), len(self.malware_label))
        self.logger.info("Chosen Size " + str(self.sample_size))
        self.logger.info("benign bm numbers " + str(len(self.benign_dict.keys())))
        self.logger.info("malware bm numbers " + str(len(self.malware_dict.keys())))

        self.data           = np.vstack((\
                                random.sample(self.benign_data, self.sample_size),\
                                random.sample(self.malware_data, self.sample_size)\
                                ))
        self.label          = np.concatenate((\
                                random.sample(self.benign_label, self.sample_size),\
                                random.sample(self.malware_label, self.sample_size)\
                                ))
        self.logger.info(self.data.shape)
        self.logger.info(self.label.shape)
        
    def setup_train_test_split(self):
        """
        Setup_train_test_split uses default split (scikit-learn split) to separate the 
        training and testing dataset.
        """
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\
                            self.data, self.label, test_size = 0.1, \
                            random_state = random.randint(0, 2**32 -1 ))

    def setup_classifiers(self):
        """
        setup_classifiers only generate the classifier class objects. nothing more.
        """
        self.names       = [\
                            "Nearest Neighbors", "Decision Tree", \
                            "Random Forest", \
                            "AdaBoost", "Naive Bayes",\
                            "Neural Net" \
                            #### "Logistic Regression",\
                            #### "Linear SVM", "Rbf SVM", "Poly SVM", "Sigmoid SVM"\
                            ####, "Gaussian Process"
                            ]
        max_iterations  = 1000
        self.classifiers = [ \
                        KNeighborsClassifier(\
                            n_neighbors=5, weights='uniform', algorithm='auto', n_jobs=-1),\
                        DecisionTreeClassifier(\
                            max_depth=100, min_samples_split=12, min_samples_leaf=12,\
                                        presort=True, max_features=None,\
                                        random_state=int(round(time.time()))),\
                        RandomForestClassifier(max_depth=100, min_samples_split=12,\
                                        min_samples_leaf=12, \
                                        n_estimators=100, max_features=None,\
                                        random_state=int(round(time.time()))), \
                        AdaBoostClassifier(algorithm='SAMME.R', n_estimators=200, \
                                        random_state=int(round(time.time()))),\
                        GaussianNB(priors=[0.5, 0.5]),\
                        MLPClassifier(hidden_layer_sizes=(100,100,100,100), \
                                        alpha=100, solver='lbfgs',\
                                        max_iter=max_iterations,\
                                        activation='tanh', tol=1e-5,\
                                        warm_start='True') \
                        #### LogisticRegression(penalty='l2', tol=1e-4, C=1e2,\
                        ####               fit_intercept=True, solver='lbfgs', \
                        ####               class_weight='balanced', max_iter=max_iterations), \
                        #### SVC(kernel="linear", C=1e2, tol=1e-4, max_iter=max_iterations,\
                        ####               probability= True),\
                        #### SVC(kernel="rbf", C=1e2, tol=1e-4, max_iter=max_iterations,\
                        ####               probability=True, shrinking=True),
                        #### SVC(kernel="poly", C=1e2, degree=4, tol=1e-4,\
                        ####               max_iter=max_iterations, probability=True),\
                        #### SVC(kernel="sigmoid", C=1e2, gamma=1e-1, tol=1e-3, \
                        ####               max_iter=max_iterations, probability=True, \
                        ####               shrinking=True)#,\
                        #### GaussianProcessClassifier(1.0 * RBF(1.0), n_jobs=-1, \
                        ####               copy_X_train=False, \
                        ####               max_iter_predict=100, warm_start=False )\
                        ]
        # self.classifiers = [ \
        #                 LogisticRegression(penalty='l2', tol=1e-9, C=1,\
        #                   fit_intercept=True, solver='lbfgs', \
        #                   class_weight='balanced', max_iter=max_iterations*100, n_jobs=-1) \
        #                 ]

    def text_class_report(self, names):
        """
        Text_class_report for experiment log files. text report updates the self.report 
        to log the results in the terminal. The results include precision, recall, f1_score
        and support. The results are dumped into classification report. The labels are 0 and 1.
        """
        #results
        precision, recall, f1_score, support = \
            precision_recall_fscore_support(self.y_test, self.prediction, \
                beta=1.0, average='binary', labels=[0, 1], pos_label=0)
        self.report[names]                  = {}
        self.report[names]['precision']     = precision
        self.report[names]['recall']        = recall
        self.report[names]['f1_score']      = f1_score
        self.report[names]['support']       = support
        self.report[names]['param']         = \
            self.classifiers[self.names.index(names)].get_params()
        
        self.logger.info("\n" + classification_report(self.y_test, self.prediction, \
            digits = 5, target_names=names))
        #self.logger.info(self.report)
        with open(self.result_path + 'classification_report.txt', 'w') as outfile:
            json.dump(self.report, outfile, indent=2)
        outfile.close()


    def roc_curve_report(self, name):
        """
        Use fpr and tpr information to record the roc information.
        """
        fpr, tpr, _         = roc_curve(self.y_test, self.predict_proba[:,0], pos_label=0)
        roc_auc             = auc(fpr, tpr)
        self.report[name]['fpr']            = list(fpr)
        self.report[name]['tpr']            = list(tpr)
        self.report[name]['roc_auc']        = roc_auc
        self.logger.info("*"*10  + str(roc_auc) + "*"*10)

        with open(self.result_path + 'classification_report.txt', 'w') as outfile:
            json.dump(self.report, outfile, indent=2)
        outfile.close()

    def run(self):
        """
        run function has the minimum experimental setup. It uses the basic training and 
        testing split.
        """
        self.logger.info('Start to run ...')
        self.load_data()
        self.setup_train_test_split()
        self.setup_classifiers()
        self.report     = {}
        for names, clf in zip(self.names, self.classifiers):
            self.logger.info(names + " in process ....")
            # train
            clf.fit(self.X_train, self.y_train)
            # if names == 'Decision Tree':
            #     tree.export_graphviz(clf, out_file=self.result_path + 'tree.dot')
            # test

            self.prediction  = clf.predict(self.X_test)
            self.text_class_report(names)

            self.predict_proba  = clf.predict_proba(self.X_test)
            self.roc_curve_report(names)

            self.logger.info(names + " testing completed!")

    def test_rasoul(self):
        """
        Test rasoul function train and test ransomware results. This evalutae the capability 
        of detecting ransomware using benignware and malware results.
        """
        self.logger.info('Start to run ...')
        self.load_data()
        self.setup_train_test_split()
        self.setup_classifiers()
        self.report     = {}
        for names, clf in zip(self.names, self.classifiers):
            self.logger.info(names + " in process ....")
            with open(self.data_path_prefix +\
                    'amd_rasoul_metadata/malware_post_pca_data.txt', 'r') as data_file:
                json_data = json.load(data_file)
            data_file.close()
            # train
            # for bm in json_data.keys():
            #     if bm == "Notepad++":
            #         for each_sample in json_data[bm]:
            #             self.X_train = np.vstack((self.X_train, each_sample))
            #             self.y_train = np.append(self.y_train, 0)
            clf.fit(self.X_train, self.y_train)
            # test
            self.X_test = []
            self.y_test = []
            for bm in json_data.keys():
                if bm == "ransomware":
                    for each_sample in json_data[bm]:
                        if self.X_test == []:
                            self.X_test = np.array(each_sample) 
                        else:
                            self.X_test = np.vstack((self.X_test, each_sample))
            for bm in json_data.keys():
                if bm == "Notepad++":
                    for each_sample in json_data[bm]:
                        self.X_test = np.vstack((self.X_test, each_sample))
            self.y_test = np.concatenate((np.ones(32),\
                                        np.zeros(32)))

            self.prediction  = clf.predict(self.X_test)
            self.text_class_report(names)

            self.predict_proba  = clf.predict_proba(self.X_test)
            self.roc_curve_report(names)

            self.logger.info(names + " testing completed!")

    def cv_text_class_report(self, names):
        """
        cv_text_class_report generates logs for cross validation results.
        """
        chunk_length            = len(self.label) / 10
        self.report[names]      = {}
        for idx in xrange(10):
            precision, recall, f1_score, support = \
                precision_recall_fscore_support(\
                self.label[idx * chunk_length : (idx + 1) * chunk_length], \
                self.prediction[idx * chunk_length : (idx + 1) * chunk_length], \
                beta=1.0, average='binary', labels=[0, 1], pos_label=0)
            self.report[names][idx]                 = {}
            self.report[names][idx]['precision']    = precision
            self.report[names][idx]['recall']       = recall
            self.report[names][idx]['f1_score']     = f1_score
            self.report[names][idx]['support']      = support
            self.logger.info("\n" + classification_report(\
                self.label[idx * chunk_length : (idx + 1) * chunk_length], \
                self.prediction[idx * chunk_length : (idx + 1) * chunk_length], \
                digits = 5, target_names=names))
        with open(self.result_path + 'cv_classification_report.txt', 'w') as outfile:
            json.dump(self.report, outfile, indent=2)
        outfile.close()
            
    def classic_cross_validation(self):
        """
        classic_cross_validation runs cross validation with ALL the data in the feature
        vectors. It does NOT separate feature vectors between training and testing dataset.
        """
        self.logger.info('Start to cross validate ...')
        self.load_data()
        self.setup_classifiers()
        self.report     = {}
        for names, clf in zip(self.names, self.classifiers):
            self.logger.info(names + " in process ....")
            # test
            zipped_list             = zip(self.data, self.label)
            random.shuffle(zipped_list)
            self.data, self.label   = zip(*zipped_list)
            self.data               = list(self.data)
            self.label              = list(self.label)
            self.prediction  = cross_val_predict(clf, self.data, self.label, cv=10)
            self.cv_text_class_report(names)

            self.logger.info(names + " testing completed!")

if __name__ == "__main__":
    """
    This is the main function.
    """
    
    logger  = logging.getLogger()
    logger.setLevel(logging.DEBUG)

    logger_handler = logging.StreamHandler()
    logger_handler.setFormatter(\
        logging.Formatter('%(asctime)s [%(levelname)s]\
            %(filename)s [%(lineno)d]: %(funcName)s(): %(message)s'))
    logger.addHandler(logger_handler)

    learn1 = boyou_learn(logger, \
            data_path_prefix='../amd_data/')
            #data_path_prefix='/home/bobzhou/2017_summer/data_analysis/amd_data_analysis/')

    learn1.result_path = sys.argv[1] + '/'
    logger.info("files will be saved in the following path: " + learn1.result_path)

    #learn1.run()
    learn1.classic_cross_validation()
    #learn1.test_rasoul()
